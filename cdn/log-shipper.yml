# cdn/log-shipper.yml
#
# NOTE: as the source LogBucket exists outside of this stack, you must create
# the S3 Event Notification trigger must be created manually.  Just select the
# "All object create events" option, and fill in the bucket/prefix.
AWSTemplateFormatVersion: "2010-09-09"
Transform: AWS::Serverless-2016-10-31

Description: >-
  Reads CloudFront logs from a Dovetail3 CDN and processes them. Anonymizes IP
  addresses and does other transformations. The resulting files are written to
  a separate S3 bucket for 3rd party usage.

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: S3 Configuration
        Parameters:
          - LogBucket
          - LogPrefix
          - DestinationBucket
          - DestinationPrefix
          - EnvironmentType
      - Label:
          default: Data Filters
        Parameters:
          - PodcastIds
    ParameterLabels:
      LogBucket:
        default: Bucket where CloudFront is logging
      LogPrefix:
        default: Path where logs are located
      DestinationBucket:
        default: Bucket to write processed log files
      DestinationPrefix:
        default: Path within destination
      EnvironmentType:
        default: Environment type
      PodcastIds:
        default: Podcast IDs to include

Parameters:
  LogBucket:
    Type: String
    Description: eg. some-bucket-name
  LogPrefix:
    Type: String
    Description: eg. some/path/here
  DestinationBucket:
    Type: String
    Description: eg. some-bucket-name
  DestinationPrefix:
    Type: String
    Description: eg. some/path/here (a trailing slash is always added)
  EnvironmentType:
    Type: String
    AllowedValues:
      - Testing
      - Staging
      - Production
  EnvironmentTypeAbbreviation:
    Type: String
    AllowedValues:
      - test
      - stag
      - prod
  PodcastIds:
    Type: String
    Description: eg. 32,85,158,99

Resources:
  ShipperFunction:
    Type: AWS::Serverless::Function
    Properties:
      Description: >-
        Process log files written by a Dovetail3 CDN and write them to a
        destination S3 bucket.
      Environment:
        Variables:
          DESTINATION_BUCKET: !Ref DestinationBucket
          DESTINATION_PREFIX: !Ref DestinationPrefix
          PODCAST_IDS: !Ref PodcastIds
      Handler: index.handler
      InlineCode: |
        const AWS = require('aws-sdk');
        const s3 = new AWS.S3();
        const zlib = require('zlib');
        const util = require('util');
        const gunzip = util.promisify(zlib.gunzip);
        const gzip = util.promisify(zlib.gzip);
        const crypto = require('crypto');

        const IPV4_MASK = /\.[0-9]{1,3}$/;
        const IPV6_MASK = /:[0-9a-fA-F]{0,4}$/;
        const maskIp = (ip, field) => {
          if (ip.match(IPV4_MASK)) {
            return ip.replace(IPV4_MASK, '.0');
          } else if (ip.endsWith(':')) {
            return ip;
          } else if (ip.match(IPV6_MASK)) {
            return ip.replace(IPV6_MASK, ':0');
          } else if (ip === '-') {
            // that's ok
          } else {
            console.warn(`Unrecognized ${field}: ${ip}`);
            return ip;
          }
        };

        const PODCAST_IDS = process.env.PODCAST_IDS.split(',').map(s => s.trim()).filter(s => s);

        exports.handler = async (event) => {
          for (const rec of event.Records) {
            const Bucket = rec.s3.bucket.name;
            const Key = rec.s3.object.key;
            const result = await s3.getObject({ Bucket, Key }).promise();
            const log = await gunzip(result.Body);
            const rows = log.toString('utf-8').split('\n').filter(r => r).map(r => r.split('\t'));

            // ensure we know what this is
            const version = rows.shift()[0];
            if (version !== '#Version: 1.0') {
              throw new Error(`Unsupported CloudFront Log Version: ${version}`);
            }

            // get fieldnames from comment
            const fields = rows.shift()[0].replace(/^#Fields: /, '').split(' ');
            const mappedRows = rows.map(row => {
              return row.reduce((acc, val, idx) => ({ ...acc, [fields[idx]]: val}), {});
            });

            // podcast id and episode guid (only works for dovetail3-cdn requests)
            const datas = mappedRows.filter(data => {
              const parts = data['cs-uri-stem'].split('/').filter(s => s);
              if (parts.length === 4) {
                data['prx-podcast-id'] = parts[0];
                data['prx-episode-guid'] = parts[1];
              } else if (parts.length === 5) {
                data['prx-podcast-id'] = parts[0];
                data['prx-episode-guid'] = parts[2];
              } else {
                console.warn(`Non-dovetail3 uri: ${data['cs-uri-stem']}`);
              }
              return PODCAST_IDS.includes(data['prx-podcast-id']);
            });
            fields.push('prx-podcast-id');
            fields.push('prx-episode-guid');

            // calculate listener_ids
            datas.forEach(data => {
              const literalIp = data['x-forwarded-for'] || data['c-ip'] || '';
              const userAgent = data['cs(User-Agent)'] || '';
              const hmac = crypto.createHmac('sha256', process.env.SECRET_KEY || '');
              hmac.update(literalIp + userAgent);
              data['prx-listener-id'] = hmac.digest('base64').replace(/\+|\/|=/g, m => {
                if (m === '+') { return '-'; }
                if (m === '/') { return '_'; }
                return '';
              });
            });
            fields.push('prx-listener-id');

            // mask IP addresses
            datas.forEach(data => {
              data['c-ip'] = maskIp(data['c-ip'], 'c-ip');
              const xffParts = (data['x-forwarded-for'] || '').split(',').map(s => s.trim()).filter(s => s);
              data['x-forwarded-for'] = xffParts.map(ip => maskIp(ip, 'x-forwarded-for')).join(', ');
            });

            // write to tsv and gzip
            const tsv = fields.join('\t') + '\n' + datas.map(data => {
              return fields.map(f => data[f] || '').join('\t');
            }).join('\n');
            const buffer = await gzip(tsv);

            // send to destination s3
            const params = {
              Bucket: process.env.DESTINATION_BUCKET,
              Key: `${process.env.DESTINATION_PREFIX}/${Key.split('/').pop()}`,
              ACL: 'bucket-owner-full-control',
              Body: buffer
            };
            if (datas.length > 0) {
              await s3.putObject(params).promise();
            }

            console.info(`Shipped ${datas.length} of ${rows.length} to s3://${params.Bucket}/${params.Key}`);
          }
        };
      MemorySize: 512
      Policies:
        - Statement:
            - Action:
                - s3:GetObject
                - s3:GetObjectVersion
              Effect: Allow
              Resource: !Sub arn:aws:s3:::${LogBucket}/${LogPrefix}/*
          Version: "2012-10-17"
        - Statement:
            - Action: s3:ListBucketMultipartUploads
              Effect: Allow
              Resource: !Sub arn:aws:s3:::${DestinationBucket}
            - Action:
                - s3:AbortMultipartUpload
                - s3:ListMultipartUploadParts
                - s3:PutObject*
              Effect: Allow
              Resource: !Sub arn:aws:s3:::${DestinationBucket}/${DestinationPrefix}/*
          Version: "2012-10-17"
      Runtime: nodejs14.x
      Tags:
        prx:meta:tagging-version: "2021-04-07"
        prx:cloudformation:stack-name: !Ref AWS::StackName
        prx:cloudformation:stack-id: !Ref AWS::StackId
        prx:ops:environment: !Ref EnvironmentType
        prx:dev:family: Dovetail
        prx:dev:application: Log Shipper
      Timeout: 30
  # TODO s3 trigger
  ShipperFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub /aws/lambda/${ShipperFunction}
      RetentionInDays: 30
      Tags:
        - { Key: prx:meta:tagging-version, Value: "2021-04-07" }
        - { Key: prx:cloudformation:stack-name, Value: !Ref AWS::StackName }
        - { Key: prx:cloudformation:stack-id, Value: !Ref AWS::StackId }
        - { Key: prx:ops:environment, Value: !Ref EnvironmentType }
        - { Key: prx:dev:family, Value: Dovetail }
        - { Key: prx:dev:application, Value: Log Shipper }
  ShipperFunctionElevatedErrorAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub ERROR [Dovetail-Log Shipper] Lambda function <${EnvironmentTypeAbbreviation}> INVOCATIONS ERRORS
      AlarmDescription: !Sub >-
        ${EnvironmentType} Dovetail Log Shipper Lambda function is failing, so
        Magellan may not be receiving log data
      ComparisonOperator: GreaterThanThreshold
      Dimensions:
        - Name: FunctionName
          Value: !Ref ShipperFunction
      EvaluationPeriods: 1
      MetricName: Errors
      Namespace: AWS/Lambda
      Period: 60
      Statistic: Sum
      Threshold: 0
      TreatMissingData: notBreaching
